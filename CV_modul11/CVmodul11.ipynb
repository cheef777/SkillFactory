{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea28b7",
   "metadata": {
    "cellId": "juk5yqcy16abpa8hdmvvp5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "533f3d8d",
   "metadata": {
    "cellId": "htyy67c2m6952e8bs78tsm",
    "id": "pCOYw0eEK9q-"
   },
   "source": [
    "# Практика сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981462a",
   "metadata": {
    "cellId": "dnpgzmncttrvvlwf7qwpfk",
    "id": "m3QgzWyX-PrQ"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Мы будем использовать Oxford-IIIT Pet Dataset. Он состоит из 37 классов собак и кошек, на каждый класс около 200 картинок. В датасете есть как боксы и маски. В датасете около 7 тысяч изображений.  \n",
    "\n",
    "![alt text](http://www.robots.ox.ac.uk/~vgg/data/pets/pet_annotations.jpg)\n",
    "\n",
    "Скачаем данные и распакуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e71c08",
   "metadata": {
    "cellId": "242ji5p5pqsfowfu0f45k9",
    "id": "JhU2bxdWaIlk"
   },
   "outputs": [],
   "source": [
    "%%capture out\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xvzf images.tar.gz && tar -xvzf annotations.tar.gz\n",
    "!rm  images/*.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f4553",
   "metadata": {
    "cellId": "d8qobc0vw4anao27yiuw0e"
   },
   "outputs": [],
   "source": [
    "!rm  images/*.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7d3e6",
   "metadata": {
    "cellId": "e5uk6ns3lvjxtd1hp3tr6",
    "id": "gvSs-KaF_7Sn"
   },
   "source": [
    "Заметим, у нас две папки в данных. Первая с `images` и вторая с разметкой масками как `binary` картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db995023",
   "metadata": {
    "cellId": "urttlsdrobj24994x6nr",
    "id": "Wv2Iz8vpuuVD"
   },
   "source": [
    "## Импортируем нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da1e16",
   "metadata": {
    "cellId": "i1gcwrkddcduy26pye5pj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9359fe",
   "metadata": {
    "cellId": "eshf6sge6wktlhoqmkklq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb15b17",
   "metadata": {
    "cellId": "vkkwn6ro3unccopw1kymp4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip install pytorch_lightning -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80282968",
   "metadata": {
    "cellId": "rzz9vp4u9q9pkjtf97c6vf",
    "id": "1MzhmOYgBpLy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e12569",
   "metadata": {
    "cellId": "rw6slbkn3soqmxi22teok",
    "id": "2kq8UgMJwPim"
   },
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8995752",
   "metadata": {
    "cellId": "zo6v8racsw3gbozxljbeb",
    "id": "Pl4EP7us71Ow"
   },
   "source": [
    "## Готовим данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e99641",
   "metadata": {
    "cellId": "twqjxpx1xvterpgfhkhxr",
    "id": "vJz_cxaf5E8S"
   },
   "source": [
    "Каждый пиксель изображения маски может принимать одно из трех значений: «1», «2» или «3». «1» означает, что данный пиксель изображения принадлежит классу «животное», «2» - классу «фон», «3» - классу «граница». Поскольку в этом примере демонстрируется задача двоичной сегментации (то есть присвоение одного из двух классов каждому пикселю), мы предварительно обработаем маску, поэтому она будет содержать только два уникальных значения: 0 если пиксель является фоном, и 1 если пиксель - это животное или граница."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44fc18",
   "metadata": {
    "cellId": "4tkicceprlmxmxegws8zm",
    "id": "u3ypFKWD3dis"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def preprocess_mask(mask):\n",
    "    mask = mask.astype(np.float32)\n",
    "    mask[mask == 2.0] = 0.0\n",
    "    mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb9bc3",
   "metadata": {
    "cellId": "c0lsgxezg1c1jzj6cj57ns",
    "id": "XnQXZatjQdnv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", transform=None):\n",
    "        images_train, images_test = train_test_split(\n",
    "            os.listdir(\"images\"), random_state=142, shuffle=True, train_size=0.8\n",
    "        )\n",
    "        if split == \"train\":\n",
    "            self.images_filenames = images_train\n",
    "        else:\n",
    "            self.images_filenames = images_test\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv2.imread(os.path.join(\"images\", image_filename))\n",
    "        if image is None:\n",
    "            return self.__getitem__(idx + 1 if self.__len__() <= idx + 1 else 0)\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(\n",
    "            os.path.join(\n",
    "                \"annotations\", \"trimaps\", image_filename.replace(\".jpg\", \".png\")\n",
    "            ),\n",
    "            cv2.IMREAD_UNCHANGED,\n",
    "        )\n",
    "        if mask is None:\n",
    "            return self.__getitem__(idx + 1 if self.__len__() <= idx + 1 else 0)\n",
    "\n",
    "        mask = preprocess_mask(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            \n",
    "        else:\n",
    "            image = torch.tensor(image).permute(2, 0, 1)\n",
    "            mask = torch.tensor(mask)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cf978",
   "metadata": {
    "cellId": "7nhjywcskml8oyrgyu6sxw",
    "id": "vTGjpVxesdeQ"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = PetDataset(split='train')\n",
    "val_dataset = PetDataset(split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c56d0",
   "metadata": {
    "cellId": "cll431dqqyryf8iz8k8bz",
    "id": "tsACO4vP6A-k"
   },
   "source": [
    "### Визуализация для проверки себя, аугментаций, данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa5c12",
   "metadata": {
    "cellId": "v3dmiji9b5b1o3uab0432pi",
    "id": "4nlyPHLB3NQE"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def display_few_examples_from_data(dataset, n=4):\n",
    "    figure, ax = plt.subplots(nrows=n, ncols=2, figsize=(10, 24))\n",
    "    for i in range(n):\n",
    "        image, mask = dataset.__getitem__(i)\n",
    "        image = torch.tensor(image).permute(1, 2, 0)\n",
    "        mask = torch.tensor(mask)\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n",
    "\n",
    "        ax[i, 0].set_title(\"Image\")\n",
    "        ax[i, 1].set_title(\"Mask\")\n",
    "\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6995c7",
   "metadata": {
    "cellId": "r6wu1yhlfwk8ebev9gh4q7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LTtgY2km3X_W",
    "outputId": "deb072a0-e30a-4367-eb8e-59712419b961"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "display_few_examples_from_data(train_dataset)\n",
    "print(\"Validation dataset\")\n",
    "display_few_examples_from_data(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67382de8",
   "metadata": {
    "cellId": "oqzb8l5i7ye5dbqype5n7",
    "id": "fG1ouoLJ6rcP"
   },
   "source": [
    "### Выбираем аугментации для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe49344",
   "metadata": {
    "cellId": "6wtdwaye55tijzwlj6ulu",
    "id": "dsz-vPRZ4mf5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.LongestMaxSize(512),\n",
    "        A.PadIfNeeded(min_height=512, min_width=512),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_dataset = PetDataset(split='train', transform=train_transform)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.LongestMaxSize(512),\n",
    "        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "val_dataset = PetDataset(split='val', transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9112211",
   "metadata": {
    "cellId": "u2mjdvz26wnavas70re03u",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7LHfo0Es6pho",
    "outputId": "4fec0724-9fe1-4f82-9b47-483e2e82ed86",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "display_few_examples_from_data(train_dataset)\n",
    "print(\"Validation dataset\")\n",
    "display_few_examples_from_data(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f508d",
   "metadata": {
    "cellId": "rhty5kmemdgqemz48g9jkg",
    "id": "QbUpOzw58FJw"
   },
   "source": [
    "## Модель и обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2902ff",
   "metadata": {
    "cellId": "fpsv2emqxkqdgu5fr6bh",
    "id": "2UmJspwxZ5I1"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def IoU(preds, targs, eps: float = 1e-8):\n",
    "    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n",
    "    Notes: [Batch size,Num classes,Height,Width]\n",
    "    Args:\n",
    "        targs: a tensor of shape [B, H, W] or [B, 1, H, W].\n",
    "        preds: a tensor of shape [B, C, H, W]. Corresponds to\n",
    "            the raw output or logits of the model. (prediction)\n",
    "        eps: added to the denominator for numerical stability.\n",
    "    Returns:\n",
    "        iou: the average class intersection over union value\n",
    "             for multi-class image segmentation\n",
    "    \"\"\"\n",
    "    num_classes = preds.shape[1]\n",
    "\n",
    "    # Single class segmentation?\n",
    "    if num_classes == 1:\n",
    "        true_1_hot = torch.eye(num_classes + 1)[targs.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "        pos_prob = torch.sigmoid(preds)\n",
    "        neg_prob = 1 - pos_prob\n",
    "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "\n",
    "    # Multi-class segmentation\n",
    "    else:\n",
    "        # Convert target to one-hot encoding\n",
    "        # true_1_hot = torch.eye(num_classes)[torch.squeeze(targs,1)]\n",
    "        true_1_hot = torch.eye(num_classes)[targs.squeeze(1)]\n",
    "\n",
    "        # Permute [B,H,W,C] to [B,C,H,W]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Take softmax along class dimension; all class probs add to 1 (per pixel)\n",
    "        probas = F.softmax(preds, dim=1)\n",
    "\n",
    "    true_1_hot = true_1_hot.type(preds.type())\n",
    "\n",
    "    # Sum probabilities by class and across batch images\n",
    "    dims = (0,) + tuple(range(2, targs.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)  # [class0,class1,class2,...]\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)  # [class0,class1,class2,...]\n",
    "    union = cardinality - intersection\n",
    "    iou = (intersection / (union + eps)).mean()  # find mean of class IoU values\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bd65e",
   "metadata": {
    "cellId": "ye5nh3hwqnscylo8fa1x"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip install ternausnet > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab9990",
   "metadata": {
    "cellId": "1jqbwmw1p9sf63r6se3ibg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import ternausnet.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b313ca",
   "metadata": {
    "cellId": "hp3yay6sq9rc7ebbwhhwa9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fa159",
   "metadata": {
    "cellId": "7i847x6a0f3df56h0hz3sj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41538354",
   "metadata": {
    "cellId": "9ixbao290692wtvftb5p7y"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    stream = tqdm(train_loader)\n",
    "    for i, (images, target) in enumerate(stream, start=1):\n",
    "        images = images.to(params[\"device\"], non_blocking=True)\n",
    "        target = target.to(params[\"device\"], non_blocking=True)\n",
    "        output = model(images).squeeze(1)\n",
    "        loss = criterion(output, target)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stream.set_description(\n",
    "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e2961",
   "metadata": {
    "cellId": "5mb7t9gxuzdjzirw5buu9r"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def validate(val_loader, model, criterion, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)\n",
    "            output = model(images).squeeze(1)\n",
    "            loss = criterion(output, target)\n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e5acd",
   "metadata": {
    "cellId": "wodvlv1pn4jx7fgp91he2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train_and_validate(model, train_dataset, val_dataset, params):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "        validate(val_loader, model, criterion, epoch, params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ce620",
   "metadata": {
    "cellId": "fw35gdd9zdsbtpx3c42ggr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "params = {\n",
    "    \"model\": \"UNet11\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f907d9",
   "metadata": {
    "cellId": "n98pd6q9qzciaa1krvets8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_unet = getattr(ternausnet.models, params[\"model\"])(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfa3f2",
   "metadata": {
    "cellId": "ptg23zuikypvan1mipcuv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_unet = model_unet.to(params[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65fb69",
   "metadata": {
    "cellId": "o0wytp21uakq5sqtbyfg4g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_unet = train_and_validate(model_unet, train_dataset, val_dataset, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337f5d5",
   "metadata": {
    "cellId": "sbofzla33x0fzygs1klif"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "figure, ax = plt.subplots(nrows=4, ncols=3, figsize=(10, 24))\n",
    "\n",
    "for i in range(4):\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    image, mask = next(iter(val_loader))\n",
    "    preds = model_unet(image.to('cuda'))\n",
    "    ax[i, 0].imshow(torch.tensor(image).squeeze(0).permute(1, 2,0))\n",
    "    ax[i, 1].imshow(torch.tensor(mask).squeeze(0), interpolation=\"nearest\")\n",
    "    ax[i, 2].imshow(preds.to('cpu').squeeze(0).permute(1, 2,0).detach().numpy(), interpolation=\"nearest\")\n",
    "    \n",
    "    ax[i, 0].set_title(\"Image\")\n",
    "    ax[i, 1].set_title(\"Mask\")\n",
    "    ax[i, 2].set_title(\"Preds\")\n",
    "    \n",
    "    ax[i, 0].set_axis_off()\n",
    "    ax[i, 1].set_axis_off()\n",
    "    ax[i, 2].set_axis_off()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bc7a8",
   "metadata": {
    "cellId": "pmwwcgsyyjohi654044lf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149109d6",
   "metadata": {
    "cellId": "6wh9lw4983i0m91tf3r6ia"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_unet.eval()\n",
    "iou_sum = 0\n",
    "val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "with torch.no_grad():\n",
    "    for image, mask in val_loader:\n",
    "        output = model_unet(image.to('cuda'))\n",
    "        iou_score = IoU(output.float(), mask.long())\n",
    "        iou_sum += iou_score\n",
    "iou_res = iou_sum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c571d",
   "metadata": {
    "cellId": "u3we57n9r4z70vvkx6eqn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "iou_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ebaa76",
   "metadata": {
    "cellId": "x21sp62klniv5opkxsiotj"
   },
   "source": [
    "## Вывод:\n",
    "IoU получилась 0.72\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "7e4e7207-5e69-4f9e-b386-f7105a0e58b2",
  "notebookPath": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
